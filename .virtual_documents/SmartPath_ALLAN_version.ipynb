











# Importing libraries
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
import numpy as np

warnings.filterwarnings('ignore')





import pandas as pd

# Defining path to the dataset folder
data_path = 'data'

# Loading core datasets 
occupation_df = pd.read_excel(f'{data_path}/Occupation Data.xlsx')
interests_df = pd.read_excel(f'{data_path}/Interests.xlsx')
skills_df = pd.read_excel(f'{data_path}/Skills.xlsx')
education_df = pd.read_excel(f'{data_path}/Education, Training, and Experience.xlsx')
related_df = pd.read_excel(f'{data_path}/Related Occupations.xlsx')
abilities_df = pd.read_excel(f'{data_path}/Abilities.xlsx')
emerging_tasks_df = pd.read_excel(f'{data_path}/Emerging Tasks.xlsx')
edu_categories_df = pd.read_excel(f'{data_path}/Education, Training, and Experience Categories.xlsx')





# View structure of each DataFrame
print("Occupations:", occupation_df.shape)
print("Interests:", interests_df.shape)
print("Skills:", skills_df.shape)
print("Education:", education_df.shape)
print("Related Jobs:", related_df.shape)
print("Abilities:", abilities_df.shape)
print("Emerging Tasks:", emerging_tasks_df.shape)
print("Education Categories:", edu_categories_df.shape)

# Display sample rows
occupation_df.head(3)


occupation_df.head()
interests_df.head()
skills_df.head()
education_df.head()
related_df.head()
abilities_df.head()





# See column names
print(occupation_df.columns)
print('\n----------\n')  
print(interests_df.columns)
print('\n----------\n')
print(skills_df.columns)
print('\n----------\n')
print(education_df.columns)
print('\n----------\n')
print(related_df.columns)
print('\n----------\n')











# Standardize 'O*NET-SOC Code' column name
for df in [occupation_df, interests_df, skills_df, education_df, related_df, emerging_tasks_df, edu_categories_df]:
    df.rename(columns={'O*NET-SOC Code': 'ONET_Code'}, inplace=True) # Renaming for clarity

# Standardize abilities columns
abilities_df = abilities_df[['O*NET-SOC Code', 'Element Name', 'Scale ID', 'Data Value']]
abilities_df.rename(columns={'O*NET-SOC Code': 'ONET_Code', 'Element Name': 'Ability'}, inplace=True)
abilities_df = abilities_df[abilities_df['Scale ID'] == 'IM']  # Only Importance scale
abilities_df.drop(columns='Scale ID', inplace=True)


# Skills Column Names
skills_df.rename(columns={
    'O*NET-SOC Code': 'ONET_Code',
    'Job Title': 'Title',
    'Element Name': 'Skill Name',
    'Scale': 'Scale ID',
    'Value': 'Data Value'
}, inplace=True)


print(skills_df.columns)














# Pivoting the interests to get one row per job with RIASEC scores
riasec_df = interests_df.pivot_table(index='ONET_Code',
                                     columns='Element Name',
                                     values='Data Value').reset_index()

riasec_df.columns.name = None
riasec_df.rename(columns={
    'Realistic': 'R',
    'Investigative': 'I',
    'Artistic': 'A',
    'Social': 'S',
    'Enterprising': 'E',
    'Conventional': 'C'
}, inplace=True)

# Preview RIASEC vectors
riasec_df.head()








# Exploring what's in skills_df
skills_df = skills_df[skills_df['Scale ID'] == 'IM']








# Filtering only 'Importance' scores
important_skills_df = skills_df[skills_df['Scale ID'] == 'IM']

# Keeping top 10 most important skills per job
top_skills_df = important_skills_df.groupby('ONET_Code', group_keys=False).apply(
    lambda x: x.sort_values(by='Data Value', ascending=False).head(10)
).reset_index(drop=True)

# Preview of top skills for one job
top_skills_df[top_skills_df['ONET_Code'] == '11-1011.00']








top_skills_df = (skills_df
    .sort_values(['ONET_Code', 'Data Value'], ascending=[True, False])
    .groupby('ONET_Code')
    .agg({'Skill Name': lambda x: list(x.head(3))})
    .rename(columns={'Skill Name': 'Top_Skills'})
    .reset_index()
)





# Filtering original `skills_df` 
onet_id = '11-1011.00'
job_skills = skills_df[skills_df['ONET_Code'] == onet_id]

# Sort top N (e.g. top 10)
job_skills = job_skills.sort_values('Data Value', ascending=False).head(10)


# Top 10 Skills for a Sample Job
plt.figure(figsize=(14, 6))
sns.barplot(x='Data Value', y='Skill Name', data=job_skills, palette='viridis')
plt.title("Top 10 Important Skills: Chief Executives")
plt.xlabel("Importance Score")
plt.ylabel("Skill")
plt.tight_layout()
plt.show()











# Filter rows related to education levels (optional if already clean)
required_edu_df = education_df[
    education_df['Element Name'].str.lower().str.contains("required level of education")
]


education_df['Element Name'].unique()


# Keep only education rows with non-zero Data Value 
required_edu_df = required_edu_df[required_edu_df['Data Value'] > 0]





# Getting the most common (highest scoring) education level per job, now with category
top_edu_df = (
    required_edu_df.sort_values(['ONET_Code', 'Data Value'], ascending=[True, False])
    .drop_duplicates(subset='ONET_Code', keep='first')  # Keep top level per job
    .loc[:, ['ONET_Code', 'Data Value', 'Category']]
    .rename(columns={'Data Value': 'Education Importance'})
)

# Preview the result
top_edu_df.head()


# Map the numeric Category to the actual education level from edu_categories_df
edu_mapping = edu_categories_df[['Category', 'Category Description']].drop_duplicates()


# Renaming for clarity
top_edu_df = top_edu_df.rename(columns={'Category Description': 'Most Common Education'})


# preview
top_edu_df = top_edu_df.reset_index(drop=True)
print(top_edu_df.head())





# Creating an Education Level Dictionary
education_level_map = {
    1.0: "Less than High School",
    2.0: "High School Diploma or equivalent",
    3.0: "Post-Secondary Certificate",
    4.0: "Some College Courses",
    5.0: "Associate's Degree",
    6.0: "Bachelor's Degree",
    7.0: "Post-Baccalaureate Certificate",
    8.0: "Master's Degree",
    9.0: "Post-Master's Certificate",
    10.0: "First Professional Degree",
    11.0: "Doctoral Degree",
    12.0: "Post-Doctoral Training"
}

edu_mapping_cleaned = pd.DataFrame(list(education_level_map.items()), columns=['Category', 'Most Common Education'])





# Use cleaned mapping (either Option 1 or 2)
top_edu_df = top_edu_df.merge(edu_mapping_cleaned, on='Category', how='left')
top_edu_df = top_edu_df.drop(columns=['Category'])  
print(top_edu_df.head())








# Creating full job profile
# Merging job_df and riasec_df
job_profiles = occupation_df.merge(riasec_df, on='ONET_Code', how='left')

# Merging education info
job_profiles = job_profiles.merge(top_edu_df, on='ONET_Code', how='left')

# Merging skills info 
job_profiles = job_profiles.merge(skills_df, on='ONET_Code', how='left')

# View sample
job_profiles.head()








# Checking for duplicates
job_profiles.duplicated().sum()





# Checking missing values across all columns
job_profiles.isnull().sum()





# Dropping rows where education info is missing
job_profiles = job_profiles.dropna(subset=['Most Common Education'])

# Dropping rows without Skill Name 
job_profiles['Skill Name'] = job_profiles['Skill Name'].fillna('Unknown')

# Fill missing RIASEC interests with 'Unknown'
ria_sec_columns = ['First Interest High-Point', 'Second Interest High-Point', 'Third Interest High-Point']
job_profiles[ria_sec_columns] = job_profiles[ria_sec_columns].fillna('Unknown')

# Dropping columns not needed
job_profiles.drop(columns=['Not Relevant'], inplace=True)

# Reset index
job_profiles.reset_index(drop=True, inplace=True)


# Preview the cleaned DataFrame
job_profiles


# Preview the cleaned DataFrame
job_profiles.info()


# Renaming 'Title_x' to 'Title'
job_profiles.rename(columns={'Title_x': 'Title'}, inplace=True)

# Dropping 'Title_y' if it's the same as 'Title_x'
job_profiles.drop(columns=['Title_y'], inplace=True)
job_profiles





from sklearn.preprocessing import MinMaxScaler

riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
scaler = MinMaxScaler()
job_profiles[riasec_cols] = scaler.fit_transform(job_profiles[riasec_cols])





# Normalize Education Score for filtering or weighted scoring.
scaler = MinMaxScaler()
job_profiles['Education Importance'] = scaler.fit_transform(job_profiles[['Education Importance']])


# Skill Score Normalization
job_profiles['Data Value Normalized'] = (
    job_profiles.groupby('Scale Name')['Data Value']
    .transform(lambda x: (x - x.min()) / (x.max() - x.min()))
)





edu_dist = job_profiles['Most Common Education'].value_counts().sort_values(ascending=False)
print(edu_dist)


import seaborn as sns

plt.figure(figsize=(14, 6))
sns.countplot(data=job_profiles, x='Most Common Education', order=edu_dist.index, palette='Set2')
plt.title('Distribution of Education Levels Across All Jobs')
plt.xlabel('Education Level')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()








# List of RIASEC dimensions
riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']

# Plotting the distribution of each RIASEC score
plt.figure(figsize=(14, 6))
sns.boxplot(data=job_profiles[riasec_cols], palette="Set3")
plt.title("Distribution of RIASEC Scores Across All Jobs", fontsize=14)
plt.ylabel("Score")
plt.xlabel("RIASEC Dimension")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()








# Mean RIASEC scores
mean_riasec = job_profiles[riasec_cols].mean().sort_values(ascending=False)

plt.figure(figsize=(12, 5))
sns.barplot(x=mean_riasec.index, y=mean_riasec.values, palette="viridis")
plt.title("Average RIASEC Scores Across All Jobs")
plt.ylabel("Average Score")
plt.xlabel("RIASEC Dimension")
plt.ylim(0, 100)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()














from sklearn.metrics.pairwise import cosine_similarity





# RIASEC scores: [Realistic, Investigative, Artistic, Social, Enterprising, Conventional]
# Sample user RIASEC profile (scale: 0–7)
user_profile = {
    'R': 3,
    'I': 6,
    'A': 4,
    'S': 5,
    'E': 2,
    'C': 6
}

# Converting to numpy array for similarity calculation
user_vector = np.array([user_profile[key] for key in ['R', 'I', 'A', 'S', 'E', 'C']]).reshape(1, -1)





# Extracting job RIASEC features
riasec_columns = ['R', 'I', 'A', 'S', 'E', 'C']
job_vectors = job_profiles[riasec_columns].values





# Computing similarity between user and all job profiles
similarities = cosine_similarity(user_vector, job_vectors).flatten()
job_profiles['Similarity Score'] = similarities

job_profiles





# Plotting
plt.figure(figsize=(14, 6))
sns.histplot(similarities, bins=30, kde=True, color='skyblue')
plt.title("Distribution of Cosine Similarities Between User and All Jobs")
plt.xlabel("Cosine Similarity Score")
plt.ylabel("Number of Jobs")
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()








# Sort jobs by similarity score
job_profiles_sorted = job_profiles.sort_values(by='Similarity Score', ascending=False)

# Drop duplicates by ONET code to get distinct jobs
top_unique_matches = job_profiles_sorted.drop_duplicates(subset='ONET_Code').head(10)

# Display key info
top_unique_matches[['ONET_Code', 'Title', 'Description', 'Similarity Score']].style.background_gradient(cmap='Greens')








# Example filter: Only Bachelor's Degree and below
user_edu_level = 3  # For example: 1 = High School, 2 = Certificate, 3 = Degree...

filtered_jobs = top_matches[top_matches['Education Importance'] <= user_edu_level]





# Defining similarity threshold
threshold = 0.80

filtered_matches = job_profiles[job_profiles['Similarity Score'] >= threshold]





import plotly.graph_objects as go

# Compare user profile and top match
top_job = top_matches.iloc[0]

labels = riasec_columns + [riasec_columns[0]]  # wrap to first value for closed shape
user_scores = list(user_profile.values()) + [user_profile['R']]
job_scores = [top_job[col] for col in riasec_columns] + [top_job['R']]

fig = go.Figure()

fig.add_trace(go.Scatterpolar(r=user_scores, theta=labels, fill='toself', name='User'))
fig.add_trace(go.Scatterpolar(r=job_scores, theta=labels, fill='toself', name=top_job['Title']))

fig.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[0, 7])),
    title="RIASEC Radar: User vs Top Matching Job",
    showlegend=True
)
fig.show()











# RIASEC scores: [Realistic, Investigative, Artistic, Social, Enterprising, Conventional]
user_profile = {
    'R': 3,
    'I': 6,
    'A': 4,
    'S': 5,
    'E': 2,
    'C': 6
}

# Converting to numpy array
user_vector = np.array([user_profile[k] for k in ['R', 'I', 'A', 'S', 'E', 'C']]).reshape(1, -1)

# Assume user's education level (as numeric - aligned with the job_profiles['Education Level'])
user_education_level = 3  # e.g., 3 = Bachelor's Degree


# Merging into job_profiles_clean
job_profiles_clean = job_profiles_clean.merge(skills_per_job, on='ONET_Code', how='left')





job_riasec_vectors = job_profiles[['R', 'I', 'A', 'S', 'E', 'C']].values





# Compute similarity scores between user and all jobs
cosine_sim_scores = cosine_similarity(user_vector, job_riasec_vectors)[0]

job_profiles['RIASEC_Similarity'] = cosine_sim_scores





# Allow user to be slightly under-qualified for more exploration
filtered_jobs = job_profiles[job_profiles['Data Value'] <= user_education_level + 1].copy()

# Filter jobs based on education level first
user_education_level = 6.0
filtered_jobs = job_profiles[job_profiles['Data Value'] <= user_education_level].copy()

# Get RIASEC vectors *from filtered data*
riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
job_vectors = filtered_jobs[riasec_cols].values

# Compute similarity with user's RIASEC vector
similarities = cosine_similarity(user_vector, job_vectors)[0]
filtered_jobs['RIASEC_Similarity'] = similarities





# Remove duplicates and filter overly similar titles
top_matches = filtered_jobs.sort_values(by='RIASEC_Similarity', ascending=False)
top_unique_matches = top_matches.drop_duplicates(subset='Title')

# Optional: limit to top N diverse results
top_results = top_unique_matches.head(10)
top_results








# Filter the Skills to Only ‘Importance’ Scale
skills_filtered = skills_df[skills_df['Scale Name'] == 'Importance']


# Aggregate All Required Skills Per Job into Sets
job_required_skills = skills_filtered.groupby('ONET_Code')['Skill Name'].apply(set).reset_index()

job_required_skills.columns = ['ONET_Code', 'Required_Skills']


# Merge With Main Job Profiles
job_profiles = job_profiles.merge(job_required_skills, on='ONET_Code', how='left')


# Compute Skill Overlap Count
def compute_skill_overlap(row, user_skills):
    if isinstance(row['Required_Skills'], set):
        return len(user_skills & row['Required_Skills'])
    return 0

# Example user skill input (replace with actual user selection)
user_skills = {'Reading Comprehension', 'Active Listening', 'Writing', 'Speaking'}

job_profiles['Skill_Overlap_Count'] = job_profiles.apply(lambda row: compute_skill_overlap(row, user_skills), axis=1)


# Recalculating Final Score with Boost for Skill Match
max_overlap = job_profiles['Skill_Overlap_Count'].max()
job_profiles['Skill_Overlap_Score'] = job_profiles['Skill_Overlap_Count'] / max_overlap  # Normalize

job_profiles['Final_Score'] = (
    0.7 * job_profiles['RIASEC_Similarity'] +
    0.3 * job_profiles['Skill_Overlap_Score']
)


# Apply threshold and display top matches
top_jobs = job_profiles[job_profiles['RIASEC_Similarity'] > 0.85]
top_jobs = top_jobs.sort_values(by='Final_Score', ascending=False).head(10)

top_jobs[['Title', 'Most Common Education', 'RIASEC_Similarity', 'Skill_Overlap_Count', 'Final_Score']]


# Step 1: Sort by Final Score
sorted_jobs = job_profiles.sort_values(by='Final_Score', ascending=False)

# Step 2: Drop duplicates by Title to avoid repetition
unique_jobs = sorted_jobs.drop_duplicates(subset='Title')

# Step 3: Apply a reasonable threshold to maintain relevance
top_diverse_jobs = unique_jobs[unique_jobs['RIASEC_Similarity'] > 0.75].head(10)

# Step 4: Show selected columns
top_diverse_jobs[['Title', 'Most Common Education', 'RIASEC_Similarity', 'Skill_Overlap_Count', 'Final_Score']]



job_profiles.columns.tolist()


from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Normalize Skill Overlap Score
job_profiles['Normalized_Skill_Score'] = scaler.fit_transform(
    job_profiles[['Skill_Overlap_Score']]
)


# Normalize Education Importance Score
job_profiles['Normalized_Education_Score'] = scaler.fit_transform(
    job_profiles[['Education Importance']]
)

job_profiles[['RIASEC_Similarity', 'Normalized_Skill_Score', 'Normalized_Education_Score']].head()


job_profiles['RIASEC_Similarity'].value_counts()





# Hybrid User Vector
job_profiles['Final_Score'] = (
    0.5 * job_profiles['RIASEC_Similarity'] +
    0.3 * job_profiles['Normalized_Skill_Score'] +
    0.2 * job_profiles['Normalized_Education_Score']
)





top_jobs = (
    job_profiles.sort_values(by='Final_Score', ascending=False)
    .drop_duplicates(subset='Title', keep='first')
)


print(top_jobs.columns.tolist())



top_jobs_display = top_jobs[
    [
        'Title', 'Description', 'Most Common Education',
        'RIASEC_Similarity', 'Skill_Overlap_Score', 'Normalized_Skill_Score',
        'Normalized_Education_Score', 'Final_Score'
    ]
].sort_values(by='Final_Score', ascending=False).head(10)

top_jobs_display.style.background_gradient(cmap='YlGn')





print(job_profiles.columns.tolist())





# Step 1: Extract relevant columns for pivoting
skills_long = job_profiles[['ONET_Code', 'Skill Name', 'Data Value Normalized']].dropna()

# Step 2: Pivot to wide format
skills_wide = skills_long.pivot_table(
    index='ONET_Code',
    columns='Skill Name',
    values='Data Value Normalized',
    fill_value=0
)

# Step 3: Rename columns for clarity
skills_wide.columns = ['Skill List_' + col.strip().replace(' ', '_') for col in skills_wide.columns]
skills_wide.reset_index(inplace=True)

# Step 4: Merge back into job_profiles
job_profiles = job_profiles.drop_duplicates(subset='ONET_Code')  # Avoid duplicates
job_profiles = job_profiles.merge(skills_wide, on='ONET_Code', how='left')



skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
print(skill_cols[:10])  # just preview a few





# Ensure skill_cols exist in both job_profiles and user vector
user_skill_vector = np.array([user_profile.get(skill.replace("Skill List_", "").replace("_", " "), 0) for skill in skill_cols])

# Create a job skill matrix from the same skill_cols
job_skill_matrix = job_profiles[skill_cols].fillna(0).values

# Dot product similarity between user and all jobs
job_profiles['Skill_Similarity_Score'] = job_skill_matrix @ user_skill_vector





from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
job_profiles['Skill_Similarity_Score_Norm'] = scaler.fit_transform(job_profiles[['Skill_Similarity_Score']])
job_profiles['Final_Score'] = (
    0.4 * job_profiles['Skill_Similarity_Score_Norm'] +
    0.3 * job_profiles['RIASEC_Similarity'] +
    0.3 * job_profiles['Normalized_Education_Score']
)


top_jobs = job_profiles.sort_values(by='Final_Score', ascending=False).head(10)
top_jobs 





user_selected_skills = ['Active Listening', 'Speaking', 'Critical Thinking']




# Clean skill names to match format in skill_cols
formatted_user_skills = ['Skill List_' + skill.replace(" ", "_") for skill in user_selected_skills]

# Build user skill vector: 1 if selected, else 0
user_skill_vector = np.array([1 if col in formatted_user_skills else 0 for col in skill_cols])


job_skill_matrix = job_profiles[skill_cols].fillna(0).values

job_profiles['Skill_Similarity_Score'] = job_skill_matrix @ user_skill_vector


## Normalize + Recompute Final Score
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
job_profiles['Skill_Similarity_Score_Norm'] = scaler.fit_transform(job_profiles[['Skill_Similarity_Score']])

# Recompute final score (assuming other components already exist)
job_profiles['Final_Score'] = (
    0.4 * job_profiles['Skill_Similarity_Score_Norm'] +
    0.3 * job_profiles['RIASEC_Similarity'] +
    0.3 * job_profiles['Normalized_Education_Score']
)


top_jobs = job_profiles.sort_values(by='Final_Score', ascending=False).head(10)

top_jobs_display = top_jobs[
    [
        'Title', 'Description', 'Most Common Education',
        'RIASEC_Similarity', 'Skill_Similarity_Score', 'Skill_Similarity_Score_Norm',
        'Normalized_Education_Score', 'Final_Score'
    ]
]

top_jobs_display.style.background_gradient(cmap='YlGn')





# Use top 10 jobs
top_jobs_plot = top_jobs_display.copy()
top_jobs_plot = top_jobs_plot.reset_index(drop=True)

# Extract scores
titles = top_jobs_plot['Title']
riasec_scores = top_jobs_plot['RIASEC_Similarity']
skill_scores = top_jobs_plot['Skill_Similarity_Score_Norm']
education_scores = top_jobs_plot['Normalized_Education_Score']

# Set positions
ind = np.arange(len(titles))
width = 0.6

# Plotting
plt.figure(figsize=(14, 7))
p1 = plt.bar(ind, riasec_scores, width, label='RIASEC Similarity')
p2 = plt.bar(ind, skill_scores, width, bottom=riasec_scores, label='Skill Similarity')
p3 = plt.bar(ind, education_scores, width, bottom=riasec_scores + skill_scores, label='Education Score')

plt.ylabel('Score Components')
plt.title('Top 10 Jobs: Final Score Decomposition')
plt.xticks(ind, titles, rotation=90)
plt.ylim(0, 3.0)
plt.legend(loc='upper right')
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()





# Ensure both dataframes have the scores and titles
option_a = top_unique_matches[['Title', 'RIASEC_Similarity']].copy()
option_b = top_jobs_display[['Title', 'Final_Score']].copy()

# Rename columns for clarity
option_a.rename(columns={'RIASEC_Similarity': 'Option_A_Score'}, inplace=True)
option_b.rename(columns={'Final_Score': 'Option_B_Score'}, inplace=True)

# Merge on Title (you can also merge on ONET_Code if needed)
merged = pd.merge(option_a, option_b, on='Title', how='outer').fillna(0)

# Sort by Option B for better alignment
merged = merged.sort_values('Option_B_Score', ascending=False).head(10)

# Plotting
x = np.arange(len(merged['Title']))  # the label locations
width = 0.35  # the width of the bars

plt.figure(figsize=(12, 6))
plt.bar(x - width/2, merged['Option_A_Score'], width, label='RIASEC Only (Option A)', color='skyblue')
plt.bar(x + width/2, merged['Option_B_Score'], width, label='Hybrid Score (Option B)', color='salmon')

plt.ylabel('Score')
plt.title('Top 10 Job Matches: RIASEC Only vs Hybrid Score')
plt.xticks(x, merged['Title'], rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()





# Prepare the data from previous step (reusing merged dataframe)
option_a = top_unique_matches[['Title', 'RIASEC_Similarity']].copy()
option_b = top_jobs_display[['Title', 'Final_Score']].copy()

# Rename for clarity
option_a.rename(columns={'RIASEC_Similarity': 'Option_A_Score'}, inplace=True)
option_b.rename(columns={'Final_Score': 'Option_B_Score'}, inplace=True)

# Merge and sort
merged = pd.merge(option_a, option_b, on='Title', how='outer').fillna(0)
merged = merged.sort_values('Option_B_Score', ascending=False).head(10)

# Plot
plt.figure(figsize=(12, 6))
plt.plot(merged['Title'], merged['Option_A_Score'], marker='o', linestyle='-', label='RIASEC Only (Option A)', color='blue')
plt.plot(merged['Title'], merged['Option_B_Score'], marker='s', linestyle='--', label='Hybrid Score (Option B)', color='green')

plt.title('Top 10 Job Matches: RIASEC Only vs Hybrid Similarity Score')
plt.ylabel('Score')
plt.xlabel('Job Title')
plt.xticks(rotation=45, ha='right')
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend()
plt.tight_layout()
plt.show()





import os
os.makedirs("data", exist_ok=True)



# Save to 'data/' directory
job_profiles.to_csv("data/job_profiles_cleaned.csv", index=False)


# engine.py

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity

# Load cleaned job profiles dataset
DATA_PATH = Path(__file__).parent / "data"
job_profiles = pd.read_csv(DATA_PATH / "job_profiles_cleaned.csv")

# Detect encoded skills
def get_encoded_skill_columns():
    return [col for col in job_profiles.columns if col.startswith("Skill List_")]

skill_cols = get_encoded_skill_columns()


# Function to compute similarity score
def compute_similarity(user_profile):
    """
    Compute final hybrid similarity score using RIASEC + Education + Skill overlap
    """

    # Create a user vector for RIASEC
    user_riasec_vector = np.array([
        user_profile["R"],
        user_profile["I"],
        user_profile["A"],
        user_profile["S"],
        user_profile["E"],
        user_profile["C"]
    ]).reshape(1, -1)

    # Extract job RIASEC vectors
    job_riasec_vectors = job_profiles[["R", "I", "A", "S", "E", "C"]].values

    # Compute cosine similarity
    riasec_similarities = cosine_similarity(user_riasec_vector, job_riasec_vectors).flatten()

    # Normalize education level (already done during preprocessing)
    job_profiles["education_similarity"] = job_profiles["Normalized_Education_Score"]

    # Skill overlap
    if user_profile["Selected Skills"]:
        for skill in skill_cols:
            job_profiles[skill] = job_profiles[skill].fillna(0)
        user_skills_binary = np.array([
            1 if skill.replace("Skill List_", "") in user_profile["Selected Skills"] else 0
            for skill in skill_cols
        ])
        job_skills_matrix = job_profiles[skill_cols].values

        # Cosine similarity for skill overlap
        skill_similarity = cosine_similarity([user_skills_binary], job_skills_matrix).flatten()
    else:
        # If no skills selected, assume neutral skill match
        skill_similarity = np.ones(len(job_profiles)) * 0.5

    # Final weighted score (you can tune weights)
    final_score = (
        0.5 * riasec_similarities +
        0.3 * job_profiles["education_similarity"].values +
        0.2 * skill_similarity
    )

    job_profiles["Hybrid_Similarity_Score"] = final_score

    return job_profiles


# Generate recommendations
def generate_recommendations(user_profile, top_n=10):
    """
    Return top N recommended job profiles for the given user profile.
    """

    scored_jobs = compute_similarity(user_profile)

    recommended = (
        scored_jobs
        .sort_values("Hybrid_Similarity_Score", ascending=False)
        .head(top_n)
        .copy()
    )

    return recommended[[
        "Title", "Description", "Hybrid_Similarity_Score",
        "Most Common Education", "Required_Skills"
    ]]





# Counting jobs before and after filtering
total_jobs = len(job_profiles)
filtered_jobs_count = len(job_profiles_clean['Normalized Education Score'] > (6/12))  # Assuming 6/12 is the threshold for Bachelor's Degree

# Bar plot
plt.bar(['Before Filter', 'After Filter'], [total_jobs, filtered_jobs_count], color=['#3498db', '#2ecc71'])
plt.title('Jobs Remaining After Education Filter')
plt.ylabel('Number of Jobs')
plt.tight_layout()
plt.savefig('images/jobs_remaining_after_education_filter.png', dpi=300)
plt.show()








abilities_df


# Defining User Skill Preferences
user_selected_skills = ['Deductive Reasoning', 'Information Ordering', 'Mathematical Reasoning']





from sklearn.metrics.pairwise import cosine_similarity
# Getting skill column names (already one-hot encoded)
skill_cols = [col for col in job_profiles_clean.columns if col.startswith('Skill List_')]

# Building user skill vector (assumes user selects all skills)
user_skill_vector = np.array([1] * len(skill_cols)).reshape(1, -1)

# Creating job skill matrix and fill any NaNs (if any)
job_skill_matrix = job_profiles_clean[skill_cols].fillna(0).values

# Computing cosine similarity
skill_similarities = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

# Adding similarity scores back to the DataFrame
job_profiles_clean['Skill Similarity'] = skill_similarities


# Creating binary matrix where 1 = job requires this user-selected ability
job_ability_matrix = abilities_df[abilities_df['Ability'].isin(user_selected_skills)].copy()

# Pivot to get 1 row per job, columns per selected ability
job_ability_matrix['has_skill'] = 1
job_ability_matrix = job_ability_matrix.pivot_table(
    index='ONET_Code',
    columns='Ability',
    values='has_skill',
    fill_value=0
).reset_index()





# Building user ability vector (all 1s since user selected all these skills)
user_ability_vector = np.array([1] * len(user_selected_skills)).reshape(1, -1)

# Creating job ability matrix
job_ability_matrix_only = job_ability_matrix[user_selected_skills].values
ability_similarities = cosine_similarity(user_ability_vector, job_ability_matrix_only)[0]

# Adding similarity back to job_ability_matrix
job_ability_matrix['Job Ability Skill Similarity'] = ability_similarities





# Merging ability similarities back into main job_profiles_clean DataFrame
job_profiles_clean = job_profiles_clean.merge(
    job_ability_matrix[['ONET_Code', 'Job Ability Skill Similarity']],
    on='ONET_Code',
    how='left'
)

# Filling any unmatched jobs with 0 similarity
job_profiles_clean['Job Ability Skill Similarity'] = job_profiles_clean['Job Ability Skill Similarity'].fillna(0)


# Checking the required columns exist (they should based on your recent output)
assert 'Similarity Score' in job_profiles_clean.columns
assert 'Normalized Education Score' in job_profiles_clean.columns
assert 'Skill Similarity' in job_profiles_clean.columns
assert 'Job Ability Skill Similarity' in job_profiles_clean.columns





job_profiles_clean['Job Hybrid Similarity Score'] = (
    job_profiles_clean['Similarity Score'] +
    job_profiles_clean['Normalized Education Score'] +
    job_profiles_clean['Skill Similarity'] +
    job_profiles_clean['Job Ability Skill Similarity']
)





# Sorting by total hybrid score including job abilities
top_job_hybrid_matches = job_profiles_clean.sort_values(
    'Job Hybrid Similarity Score',
    ascending=False
).head(10)

# Display top job matches with key info
top_job_hybrid_matches[[
    'Title',
    'Description',
    'Education Level',
    'Preparation Level',
    'Education Category Label',
    'Job Hybrid Similarity Score',
    'Hybrid Similarity',
    'Similarity Score',
    'Normalized Education Score'
]].style.background_gradient(cmap='YlGn')











def get_user_profile():
    """
    Collect user profile input: RIASEC scores, education level, and selected skills.
    Returns:
        dict: User profile with RIASEC scores, education level (numeric), and list of skills.
    """

    # Step 1: RIASEC Input
    print("Enter your RIASEC Scores (scale of 0–7, separated by commas):")
    print("Format: R, I, A, S, E, C")
    r_i_a_s_e_c = input("Enter scores: ").strip().split(',')

    try:
        r, i, a, s, e, c = [float(score.strip()) for score in r_i_a_s_e_c]
    except ValueError:
        print("⚠️ Invalid RIASEC input. Defaulting to neutral scores.")
        r, i, a, s, e, c = [4.0] * 6

    # Step 2: Education Level Selection
    education_map = {
        1: "Less than High School",
        2: "High School Diploma or Equivalent",
        3: "Post-Secondary Certificate",
        4: "Some College Courses",
        5: "Associate Degree",
        6: "Bachelor's Degree",
        7: "Post-Baccalaureate's Degree",
        8: "Master's Degree",
        9: "Post-Master's Certificate",
        10: "First Professional Degree",
        11: "Doctoral Degree",
        12: "Post-Doctoral Training"
    }

    print("\nSelect your highest education level:")
    for k, v in education_map.items():
        print(f"{k}. {v}")
    
    edu_input = input("Enter the number corresponding to your education level: ").strip()
    try:
        education_level = int(edu_input)
        if education_level not in education_map:
            raise ValueError
    except ValueError:
        print("⚠️ Invalid input. Defaulting to Bachelor's Degree.")
        education_level = 6

    # Step 3: Skill Selection (up to 10)
    skill_map = {
        "1"  : 'Oral Comprehension',
        "2"  : 'Written Comprehension',
        "3"  : 'Oral Expression',
        "4"  : 'Written Expression',
        "5"  : 'Fluency of Ideas',
        "6"  : 'Originality',
        "7"  : 'Problem Sensitivity',
        "8"  : 'Deductive Reasoning',
        "9"  : 'Inductive Reasoning',
        "10" : 'Information Ordering',
        "11" : 'Category Flexibility',
        "12" : 'Mathematical Reasoning',
        "13" : 'Number Facility',
        "14" : 'Memorization',
        "15" : 'Speed of Closure',
        "16" : 'Flexibility of Closure',
        "17" : 'Perceptual Speed',
        "18" : 'Spatial Orientation',
        "19" : 'Visualization',
        "20" : 'Selective Attention',
        "21" : 'Time Sharing',
        "22" : 'Arm-Hand Steadiness',
        "23" : 'Manual Dexterity',
        "24" : 'Finger Dexterity',
        "25" : 'Control Precision',
        "26" : 'Multilimb Coordination',
        "27" : 'Response Orientation',
        "28" : 'Rate Control',
        "29" : 'Reaction Time',
        "30" : 'Wrist-Finger Speed',
        "31" : 'Speed of Limb Movement',
        "32" : 'Static Strength',
        "33" : 'Explosive Strength',
        "34" : 'Dynamic Strength',
        "35" : 'Trunk Strength',
        "36" : 'Stamina',
        "37" : 'Extent Flexibility',
        "38" : 'Dynamic Flexibility',
        "39" : 'Gross Body Coordination',
        "40" : 'Gross Body Equilibrium',
        "41" : 'Near Vision',
        "42" : 'Far Vision',
        "43" : 'Visual Color Discrimination',
        "44" : 'Night Vision',
        "45" : 'Peripheral Vision',
        "46" : 'Depth Perception',
        "47" : 'Glare Sensitivity',
        "48" : 'Hearing Sensitivity',
        "49" : 'Auditory Attention',
        "50" : 'Sound Localization',
        "51" : 'Speech Recognition',
        "52" : 'Speech Clarity'
    }

    print("\nSelect up to 10 skills you consider strong:")
    for k, v in skill_map.items():
        print(f"{k}. {v}")

    skill_input = input("Enter skill numbers separated by commas (e.g., 1,8,12): ").strip()
    user_skills = []
    if skill_input:
        user_skills = [skill_map[num.strip()] for num in skill_input.split(',') if num.strip() in skill_map]

    print("\nProfile Captured. Generating personalized recommendations...\n")

    return {
        'R': r, 'I': i, 'A': a, 'S': s, 'E': e, 'C': c,
        'education_level': education_level,
        'skills': user_skills
    }














def generate_recommendations(user_profile, job_profiles_clean):
    """
    Generate top 10 job recommendations based on RIASEC, education, and skill similarity.
    """

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_vector = np.array([
        user_profile['R'], user_profile['I'], user_profile['A'],
        user_profile['S'], user_profile['E'], user_profile['C']
    ]).reshape(1, -1)

    job_vectors = job_profiles_clean[riasec_cols].values
    riasec_similarities = cosine_similarity(user_vector, job_vectors)[0]
    job_profiles_clean['User RIASEC Similarity'] = riasec_similarities

    # Education Match Score - already included as 'Normalized Education Score' (0 to 1 scale)

    # Skill Similarity
    skill_cols = [col for col in job_profiles_clean.columns if col.startswith("Skill List_")]
    
    # Converting user skill names to vector
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))

    for i, skill in enumerate(skill_cols):
        skill_name = skill.replace("Skill List_", "").lower()
        if any(skill_name in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1

    job_skill_matrix = job_profiles_clean[skill_cols].fillna(0).values
    skill_similarities = cosine_similarity(user_skill_vector, job_skill_matrix)[0]
    job_profiles_clean['User Skill Similarity'] = skill_similarities

    # Final Hybrid Score 
    job_profiles_clean['Hybrid Recommendation Score'] = (
        job_profiles_clean['User RIASEC Similarity'] +
        job_profiles_clean['Normalized Education Score'] +
        job_profiles_clean['User Skill Similarity']
    )

    # Top 10 Recommendations 
    top_matches = job_profiles_clean.sort_values('Hybrid Recommendation Score', ascending=False).head(10)

    # Returning selected columns for display
    return top_matches[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'Normalized Education Score', 'User Skill Similarity'
    ]]





# Collecting user input
user = get_user_profile()

# Generating recommendations based on user input and your cleaned job dataset
recommendations = generate_recommendations(user, job_profiles_clean)
recommendations.style.background_gradient(cmap='YlGn')





top_matches = generate_recommendations(user_profile, job_profiles_clean)

get_ipython().run_line_magic("matplotlib", " inline")

def plot_recommendation_breakdown(top_matches):
    """
    Plots the breakdown of Hybrid Recommendation Score for top job matches.
    """
    import matplotlib.pyplot as plt

    # Select and format relevant data
    plot_data = top_matches[[
        'Title', 'User RIASEC Similarity', 'Normalized Education Score', 'User Skill Similarity'
    ]].copy()
    
    plot_data.set_index('Title', inplace=True)
    plot_data.sort_values(by='User RIASEC Similarity', ascending=False, inplace=True)

    # Plot settings
    ax = plot_data.plot(kind='bar', stacked=True, figsize=(14, 8), colormap='YlGnBu')
    plt.title('📊 Hybrid Recommendation Score Breakdown for Top Jobs', fontsize=14)
    plt.ylabel('Total Score')
    plt.xlabel('Job Title')
    plt.xticks(rotation=45, ha='right')
    plt.legend(loc='lower right', bbox_to_anchor=(1.15, 1))
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()


job_profiles_with_skills = job_profiles_clean.copy()

# Save
output_path_1 = 'data/job_profiles_with_skills.csv'
job_profiles_with_skills.to_csv(output_path_1, index=False)











# Importing clustering libraries
import hdbscan
from sklearn.cluster import KMeans, DBSCAN
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA

# saving the job abilities and skills dataset that will be used for modelling
output_path_1 = 'data/job_profiles_with_skills.csv'
job_profiles_with_skills.to_csv(output_path_1, index=False)





# Displaying all column names with their index positions
for i, col in enumerate(job_profiles_with_skills.columns):
    print(f"{i}: {col}")





def generate_recommendations(user_profile, job_profiles_clean, top_n=10):
    """
    Generate top job recommendations based on RIASEC, education, and skill similarity.
    
    Parameters:
    - user_profile: dict with keys ['R', 'I', 'A', 'S', 'E', 'C', 'education_level', 'skills']
    - job_profiles_clean: DataFrame with job info and skill indicators
    - top_n: number of job recommendations to return
    
    Returns:
    - DataFrame of top recommended jobs
    """

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_vector = np.array([
        user_profile['R'], user_profile['I'], user_profile['A'],
        user_profile['S'], user_profile['E'], user_profile['C']
    ]).reshape(1, -1)

    job_vectors = job_profiles_clean[riasec_cols].values
    job_profiles_clean['User RIASEC Similarity'] = cosine_similarity(user_vector, job_vectors)[0]

    # Education Score (already normalized, assumed 0 to 1)
    # Column: 'Normalized Education Score'

    # Skill Similarity (based on user input and binary columns)
    skill_cols = [col for col in job_profiles_clean.columns if col.startswith("Skill List_")]
    user_skills = [s.lower() for s in user_profile.get('skills', [])]

    # Creating user skill vector (binary 1/0)
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, skill_col in enumerate(skill_cols):
        skill_name = skill_col.replace("Skill List_", "").lower()
        if skill_name in user_skills:
            user_skill_vector[0][i] = 1

    job_skill_matrix = job_profiles_clean[skill_cols].fillna(0).values
    job_profiles_clean['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Score
    job_profiles_clean['Hybrid Recommendation Score'] = (
        job_profiles_clean['User RIASEC Similarity'] +
        job_profiles_clean['Normalized Education Score'] +
        job_profiles_clean['User Skill Similarity']
    )

    # Top Recommendations
    top_matches = job_profiles_clean.sort_values('Hybrid Recommendation Score', ascending=False).head(top_n)

    # Return summary
    return top_matches[[  
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'Normalized Education Score', 'User Skill Similarity'
    ]].style.background_gradient(cmap='YlGn')





user_profile = {
    'R': 5, 'I': 4, 'A': 2, 'S': 3, 'E': 1, 'C': 2,
    'education_level': 10,  # On a 0–12 scale
    'skills': ['on-the-job training', 'related work experience', 'professional certification']
}

recommendations = generate_recommendations(user_profile, job_profiles_clean)
recommendations





from sklearn.cluster import AgglomerativeClustering

def agglomerative_recommender(user_profile: dict,
                               job_profiles: pd.DataFrame,
                               n_clusters: int = 10,
                               top_n: int = 5) -> pd.DataFrame:
    """
    Recommends jobs using Agglomerative Clustering and includes individual similarity components.
    """

    # Defining features
    feature_cols = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
                   job_profiles.columns[27:-1].tolist()
    X = job_profiles[feature_cols].fillna(0).values

    # Clustering
    model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
    labels = model.fit_predict(X)
    job_profiles['Cluster'] = labels

    # Normalizing and building user vector
    user_vector = np.array([
        user_profile['R'] / 7, user_profile['I'] / 7, user_profile['A'] / 7,
        user_profile['S'] / 7, user_profile['E'] / 7, user_profile['C'] / 7,
        user_profile['education_level'] / 12
    ] + [0] * (X.shape[1] - 7)).reshape(1, -1)

    # Assigning to nearest cluster
    cluster_centroids = [X[labels == i].mean(axis=0) for i in range(n_clusters)]
    cluster_similarities = cosine_similarity(user_vector, cluster_centroids)[0]
    user_cluster = np.argmax(cluster_similarities)

    # Selecting jobs in user's cluster
    cluster_jobs = job_profiles[job_profiles['Cluster'] == user_cluster].copy()

    # Similarities (component-wise) RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_riasec = np.array([[user_profile[col] / 7 for col in riasec_cols]])
    job_riasec = cluster_jobs[riasec_cols].fillna(0).values
    cluster_jobs['User RIASEC Similarity'] = cosine_similarity(user_riasec, job_riasec)[0]

    # Skill Similarity
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, col in enumerate(skill_cols):
        if any(col.replace("Skill List_", "").lower() in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1
    job_skill_matrix = cluster_jobs[skill_cols].fillna(0).values
    cluster_jobs['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Scores
    cluster_jobs['Hybrid Recommendation Score'] = (
        cluster_jobs['User RIASEC Similarity'] +
        cluster_jobs['Normalized Education Score'] +
        cluster_jobs['User Skill Similarity']
    )

    # Cosine similarity over full feature vector 
    sim_scores = cosine_similarity(user_vector, cluster_jobs[feature_cols].values).flatten()
    cluster_jobs['Similarity Score'] = sim_scores

    # Sorting and returning top results
    top_jobs = cluster_jobs.sort_values(by='Hybrid Recommendation Score', ascending=False).head(top_n)

    return top_jobs[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'User Skill Similarity',
        'Normalized Education Score', 'Similarity Score'
    ]].style.background_gradient(cmap='YlGn')





# User Input
user_profile = {
    'R': 5, 'I': 3, 'A': 4, 'S': 2, 'E': 1, 'C': 1,
    'education_level': 10,
    'skills': ['professional certification', 'on-the-job training']
}

# Getting Recommendations
agglo_recommendations = agglomerative_recommender(user_profile, job_profiles_with_skills)
agglo_recommendations








from sklearn.cluster import DBSCAN

def dbscan_recommender(user_profile: dict, 
                       job_profiles: pd.DataFrame,
                       eps: float = 0.5,
                       min_samples: int = 5,
                       top_n: int = 5) -> pd.DataFrame:
    """
    Recommends jobs using DBSCAN clustering and includes RIASEC, skills, and education match scores.
    """

    # Defining features to cluster on
    feature_cols = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
                   job_profiles.columns[27:-1].tolist()
    X = job_profiles[feature_cols].fillna(0).values

    # Fitting DBSCAN model
    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')
    labels = dbscan.fit_predict(X)
    job_profiles['Cluster'] = labels

    # Preparing user vector
    user_vector = np.array([
        user_profile['R'] / 7, user_profile['I'] / 7, user_profile['A'] / 7,
        user_profile['S'] / 7, user_profile['E'] / 7, user_profile['C'] / 7,
        user_profile['education_level'] / 12
    ] + [0] * (X.shape[1] - 7)).reshape(1, -1)

    # Selecting jobs from non-noise clusters
    valid_jobs = job_profiles[job_profiles['Cluster'] != -1].copy()

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_riasec = np.array([[user_profile[col] / 7 for col in riasec_cols]])
    job_riasec = valid_jobs[riasec_cols].fillna(0).values
    valid_jobs['User RIASEC Similarity'] = cosine_similarity(user_riasec, job_riasec)[0]

    # Skill Similarity
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, col in enumerate(skill_cols):
        if any(col.replace("Skill List_", "").lower() in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1
    job_skill_matrix = valid_jobs[skill_cols].fillna(0).values
    valid_jobs['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Score
    valid_jobs['Hybrid Recommendation Score'] = (
        valid_jobs['User RIASEC Similarity'] +
        valid_jobs['Normalized Education Score'] +
        valid_jobs['User Skill Similarity']
    )

    # Cosine similarity with full vector 
    sim_scores = cosine_similarity(user_vector, valid_jobs[feature_cols].values).flatten()
    valid_jobs['Similarity Score'] = sim_scores

    # Top N jobs
    top_jobs = valid_jobs.sort_values('Hybrid Recommendation Score', ascending=False).head(top_n)

    return top_jobs[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'User Skill Similarity',
        'Normalized Education Score', 'Similarity Score'
    ]].style.background_gradient(cmap='YlGn')





# Sample user profile
user_profile = {
    'R': 5, 'I': 3, 'A': 4, 'S': 2, 'E': 1, 'C': 1,
    'education_level': 10,
    'skills': ['on-the-job training', 'professional certification']
}

# Get recommendations from DBSCAN
dbscan_recommendations = dbscan_recommender(user_profile, job_profiles_with_skills)
dbscan_recommendations








import hdbscan

def hdbscan_recommender(user_profile: dict, 
                        job_profiles: pd.DataFrame,
                        min_cluster_size: int = 10,
                        top_n: int = 5) -> pd.DataFrame:
    """
    Recommends jobs using HDBSCAN clustering and hybrid similarity scoring (RIASEC + skills + education).
    """

    # Selecting Features for Clustering
    feature_cols = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
                   job_profiles.columns[27:-1].tolist()
    X = job_profiles[feature_cols].fillna(0).values

    # HDBSCAN Clustering
    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)
    labels = hdb.fit_predict(X)
    job_profiles['Cluster'] = labels

    # Preparing user vector
    user_vector = np.array([
        user_profile['R'] / 7, user_profile['I'] / 7, user_profile['A'] / 7,
        user_profile['S'] / 7, user_profile['E'] / 7, user_profile['C'] / 7,
        user_profile['education_level'] / 12
    ] + [0] * (X.shape[1] - 7)).reshape(1, -1)

    # Filtering out noise
    clustered_jobs = job_profiles[job_profiles['Cluster'] != -1].copy()

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_riasec = np.array([[user_profile[col] / 7 for col in riasec_cols]])
    job_riasec = clustered_jobs[riasec_cols].fillna(0).values
    clustered_jobs['User RIASEC Similarity'] = cosine_similarity(user_riasec, job_riasec)[0]

    # Skill Similarity
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, col in enumerate(skill_cols):
        if any(col.replace("Skill List_", "").lower() in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1
    job_skill_matrix = clustered_jobs[skill_cols].fillna(0).values
    clustered_jobs['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Score
    clustered_jobs['Hybrid Recommendation Score'] = (
        clustered_jobs['User RIASEC Similarity'] +
        clustered_jobs['Normalized Education Score'] +
        clustered_jobs['User Skill Similarity']
    )

    # Cosine Similarity with full feature vector
    sim_scores = cosine_similarity(user_vector, clustered_jobs[feature_cols].values).flatten()
    clustered_jobs['Similarity Score'] = sim_scores

    # Top N Jobs
    top_jobs = clustered_jobs.sort_values('Hybrid Recommendation Score', ascending=False).head(top_n)

    return top_jobs[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'User Skill Similarity',
        'Normalized Education Score', 'Similarity Score'
    ]].style.background_gradient(cmap='YlGn')





# Sample user input
user_profile = {
    'R': 5, 'I': 3, 'A': 4, 'S': 2, 'E': 1, 'C': 1,
    'education_level': 10,
    'skills': ['professional certification', 'on-the-job training']
}

# Generating recommendations
hdbscan_recommendations = hdbscan_recommender(user_profile, job_profiles_with_skills)
hdbscan_recommendations








from sklearn.decomposition import PCA

# Selecting Features for PCA
pca_features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
               job_profiles_with_skills.columns[27:-1].tolist()

X = job_profiles_with_skills[pca_features].fillna(0).values

# Applying PCA
pca = PCA(n_components=2)  # Reduce to 2 components for visualization
X_pca = pca.fit_transform(X)

# Saving PCA Components to DataFrame
job_profiles_with_skills['PCA1'] = X_pca[:, 0]
job_profiles_with_skills['PCA2'] = X_pca[:, 1]

# Adding Clustering Labels 
if 'Cluster' not in job_profiles_with_skills.columns:
    from sklearn.cluster import KMeans
    km_model = KMeans(n_clusters=8, random_state=42)
    job_profiles_with_skills['Cluster'] = km_model.fit_predict(X)





# Visualizing the PCA Output
plt.figure(figsize=(10, 6))
sns.scatterplot(
    x='PCA1', y='PCA2',
    hue='Cluster',
    palette='Set2',
    data=job_profiles_with_skills,
    legend='full',
    alpha=0.7
)
plt.title('PCA of Job Profiles Based on Skills + RIASEC + Education')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.savefig('images/job_profiles_pca.png', dpi=300)
plt.show()








print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total variance explained:", pca.explained_variance_ratio_.sum())








from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

def visualize_all_clusters(job_profile_with_skills):
    """
    Visualize and compare the clustering results from KMeans, Agglomerative, DBSCAN, and HDBSCAN.
    Prints clustering performance metrics and displays scatter plots of the clusters.
    """

    # Selecting RIASEC + Education + Skill-related columns for clustering
    features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
               job_profile_with_skills.columns[27:-1].tolist()
    
    # Converting to matrix and handle missing values
    X = job_profile_with_skills[features].fillna(0).values

    # Defining clustering models
    clusterers = {
        'KMeans': KMeans(n_clusters=10, random_state=42),
        'Agglomerative': AgglomerativeClustering(n_clusters=10),
        'DBSCAN': DBSCAN(eps=1.0, min_samples=5),
        'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=5)
    }

    # Prepare 2x2 subplot layout
    fig, axs = plt.subplots(2, 2, figsize=(14, 12))
    axs = axs.flatten()

    for i, (name, model) in enumerate(clusterers.items()):
        # Predicting clusters
        labels = model.fit_predict(X)

        # Visualizing raw clusters using first two features (note: PCA is better for visual clarity)
        axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=10)
        axs[i].set_title(f"{name} Clusters (Raw Feature Space)")
        axs[i].set_xlabel("Feature 1")
        axs[i].set_ylabel("Feature 2")

        # Calculating and displaying clustering evaluation metrics
        unique_labels = set(labels)
        if len(unique_labels) > 1 and -1 not in unique_labels:
            sil = silhouette_score(X, labels)
            db = davies_bouldin_score(X, labels)
            ch = calinski_harabasz_score(X, labels)

            print(f"\n📊 {name} Metrics:")
            print(f"  • Silhouette Score: {sil:.4f}")
            print(f"  • Davies-Bouldin Index: {db:.4f}")
            print(f"  • Calinski-Harabasz Score: {ch:.2f}")
        else:
            print(f"\n {name} produced one cluster or contains noise (unclustered points).")

    plt.tight_layout()
    plt.show()


visualize_all_clusters(job_profiles_with_skills)














from sklearn.metrics import silhouette_score

# Defining features for clustering
features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + job_profiles_with_skills.columns[27:-1].tolist()
X = job_profiles_with_skills[features].fillna(0).values

# Storing scores
inertia_scores = []
silhouette_scores = []
k_range = range(2, 15)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    inertia_scores.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, labels))

# Plotting Inertia (Elbow Curve)
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(k_range, inertia_scores, marker='o')
plt.title("Elbow Method (Inertia)")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia")

# Plotting Silhouette Scores
plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, marker='s', color='green')
plt.title("Silhouette Score by Cluster Count")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")

plt.tight_layout()
plt.show()








from sklearn.decomposition import PCA

# Reducing to 2 or 3 components for visualization or faster clustering
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total variance explained:", sum(pca.explained_variance_ratio_))








# Choosing optimal k based on plots (e.g., 8)
optimal_k = 8
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)
labels = kmeans_final.fit_predict(X_reduced)

# Adding cluster labels back to dataset
job_profiles_with_skills['KMeans Cluster (PCA)'] = labels

# Visualizing clusters
plt.figure(figsize=(14, 6))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap='Set1', s=10)
plt.title(f"K-Means Clustering (k={optimal_k}) on PCA-Reduced Data")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()





from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

sil = silhouette_score(X_reduced, labels)
db = davies_bouldin_score(X_reduced, labels)
ch = calinski_harabasz_score(X_reduced, labels)

print(f"Final KMeans Metrics (k={optimal_k}):")
print(f"Silhouette Score: {sil:.4f}")
print(f"Davies-Bouldin Index: {db:.4f}")
print(f"Calinski-Harabasz Score: {ch:.2f}")











# Defining features for PCA (RIASEC + Education + Skills)
features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
           job_profiles_with_skills.columns[27:-1].tolist()

# Preparing feature matrix
X = job_profiles_with_skills[features].fillna(0).values

# Fitting PCA and reduce to 2 components
pca_model = PCA(n_components=2)
X_reduced = pca_model.fit_transform(X)





def final_kmeans_recommender(user_profile: dict, job_profiles: pd.DataFrame, n_clusters: int = 8, top_n: int = 5):
    """
    Uses optimized KMeans clustering on PCA-reduced features AND computes individual similarity scores.
    """

    # Feature preparation 
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + skill_cols
    X = job_profiles[features].fillna(0).values

    #  PCA transformation 
    pca_model = PCA(n_components=2)
    X_reduced = pca_model.fit_transform(X)

    # Fitting KMeans on reduced features 
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_reduced)
    job_profiles = job_profiles.copy()
    job_profiles['Cluster'] = cluster_labels

    # Creating user vector (original and reduced) 
    education_score = user_profile['education_level'] / 12
    user_raw_vector = np.array([
        user_profile['R']/7, user_profile['I']/7, user_profile['A']/7,
        user_profile['S']/7, user_profile['E']/7, user_profile['C']/7,
        education_score
    ])

    # Skill vector
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros(len(skill_cols))
    for i, skill in enumerate(skill_cols):
        skill_name = skill.replace("Skill List_", "").lower()
        if any(skill_name in s.lower() for s in user_skills):
            user_skill_vector[i] = 1

    # Full vector
    user_full_vector = np.concatenate([user_raw_vector, user_skill_vector]).reshape(1, -1)

    # PCA-reduced
    user_vector_reduced = pca_model.transform(user_full_vector)

    #  Predicting user cluster
    user_cluster = kmeans.predict(user_vector_reduced)[0]
    cluster_jobs = job_profiles[job_profiles['Cluster'] == user_cluster].copy()

    # Computeing cosine similarity in PCA space 
    cluster_X_reduced = X_reduced[cluster_jobs.index]
    cosine_sim = cosine_similarity(user_vector_reduced, cluster_X_reduced).flatten()
    cluster_jobs['Cosine Similarity'] = cosine_sim

    # Computeing individual similarity scores

    # RIASEC
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    job_riasec = cluster_jobs[riasec_cols].values
    user_riasec = np.array([
        user_profile['R'], user_profile['I'], user_profile['A'],
        user_profile['S'], user_profile['E'], user_profile['C']
    ]).reshape(1, -1)
    riasec_sim = cosine_similarity(user_riasec, job_riasec)[0]
    cluster_jobs['User RIASEC Similarity'] = riasec_sim

    # Skill similarity
    job_skill_matrix = cluster_jobs[skill_cols].fillna(0).values
    skill_sim = cosine_similarity(user_skill_vector.reshape(1, -1), job_skill_matrix)[0]
    cluster_jobs['User Skill Similarity'] = skill_sim

    # Adding back normalized education (already in dataset)

    #  Final Hybrid Score 
    cluster_jobs['Hybrid Score'] = (
        cluster_jobs['User RIASEC Similarity'] +
        cluster_jobs['User Skill Similarity'] +
        cluster_jobs['Normalized Education Score']
    )

    # Returning top N recommendations
    top_matches = cluster_jobs.sort_values(by='Hybrid Score', ascending=False).head(top_n)

    return top_matches[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label',
        'Hybrid Score', 'User RIASEC Similarity', 'Normalized Education Score',
        'User Skill Similarity', 'Cosine Similarity'
    ]]





# User Input
user_profile = {
    'R': 5, 'I': 4, 'A': 2, 'S': 3, 'E': 1, 'C': 2,
    'education_level': 10,
    'skills': ['job-related professional certification']
}

final_recommendations = final_kmeans_recommender(
    user_profile, job_profiles_with_skills, n_clusters=8
)

final_recommendations.style.background_gradient(cmap='YlGn')





import os
import pickle

# Creating the directory if it doesn't exist
os.makedirs("models", exist_ok=True)

# Saving the model
with open("models/kmeans_model.pkl", "wb") as f:
    pickle.dump(kmeans, f)

# Loading the model later
with open("models/kmeans_model.pkl", "rb") as f:
    kmeans = pickle.load(f)


import os

# Create the 'data' directory if it doesn't exist
os.makedirs("data", exist_ok=True)

# Save the DataFrame
job_profiles_clean.to_csv("data/job_profiles_clean.csv", index=False)

print("job_profiles_clean.csv saved successfully!")


print("job_profiles_clean columns:\n", job_profiles_clean.columns)
print("skills_encoded columns:\n", skills_encoded.columns)


from sklearn.preprocessing import MultiLabelBinarizer
import pandas as pd

# Step 1: Load original skills data
skills_df = pd.read_excel("data/Skills.xlsx")

# Step 2: Group skills by O*NET-SOC Code (which maps to job ID)
skills_grouped = skills_df.groupby('O*NET-SOC Code')['Element Name'].apply(list).reset_index()
skills_grouped.columns = ['Job_ID', 'Skill List']

# Step 3: One-hot encode using MultiLabelBinarizer
mlb = MultiLabelBinarizer()
skills_encoded = pd.DataFrame(
    mlb.fit_transform(skills_grouped['Skill List']),
    columns=[f"Skill List_{s}" for s in mlb.classes_]
)
skills_encoded['Job_ID'] = skills_grouped['Job_ID']

#  Rename Job_ID to match ONET_Code in job_profiles_clean
skills_encoded.rename(columns={'Job_ID': 'ONET_Code'}, inplace=True)

# Step 4: Load job_profiles_clean and merge with encoded skills
job_profiles_clean = pd.read_csv("data/job_profiles_clean.csv")

# Perform the merge on ONET_Code
job_profiles_clean = pd.merge(job_profiles_clean, skills_encoded, on='ONET_Code', how='left')

# Step 5: Fill any NaNs in skill columns with 0
skill_cols = [col for col in job_profiles_clean.columns if col.startswith("Skill List_")]
job_profiles_clean[skill_cols] = job_profiles_clean[skill_cols].fillna(0)

# Step 6: Save the updated file
job_profiles_clean.to_csv("data/job_profiles_clean.csv", index=False)

# Step 7: Output summary
print(" One-hot encoded skills added successfully.")
print(" Skill columns added:", skill_cols[:5], "… total:", len(skill_cols))



print("Merged dataset shape:", job_profiles_clean.shape)
print("Sample job with encoded skills:\n", job_profiles_clean[skill_cols + ['Title']].head(1))





















